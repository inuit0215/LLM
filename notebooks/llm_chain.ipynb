{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ログレベルの設定\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドキュメントの読み込み\n",
    "with open(\"../source.txt\") as f:\n",
    "    test_all = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# チャンクの分割\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,  # チャンクの最大文字数\n",
    "    chunk_overlap=20,  # オーバーラップの最大文字数\n",
    ")\n",
    "texts = text_splitter.split_text(test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Arsenal Fo : 297\n",
      "Community  : 202\n",
      "Arsenal wa : 289\n",
      "in English : 298\n",
      "Cups, incl : 215\n",
      "In 1886, m : 296\n",
      "fortunes o : 298\n",
      "the longes : 256\n",
      "In 2006, t : 297\n",
      "media.[9]  : 99\n"
     ]
    }
   ],
   "source": [
    "# チャンクの確認\n",
    "print(len(texts))\n",
    "for text in texts:\n",
    "    print(text[:10].replace(\"\\n\", \"\\\\n\"), \":\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# インデックスの作成\n",
    "# index = FAISS.from_texts(\n",
    "#     texts=texts,\n",
    "#     embedding=HuggingFaceEmbeddings(\n",
    "#         model_name=\"intfloat/multilingual-e5-large\", cache_folder=\"./model\"\n",
    "#     ),\n",
    "# )\n",
    "# index.save_local(\"storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivm-2023-01/PythonApp/LLM/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: ./model/intfloat_multilingual-e5-large\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n",
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "# インデックスの読み込み\n",
    "index = FAISS.load_local(\n",
    "    \"storage\",\n",
    "    HuggingFaceEmbeddings(model_name=\"../model/intfloat_multilingual-e5-large\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# Verbose is required to pass to the callback manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ./model/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 1.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 3917.73 MB (+  256.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  =  256.00 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# LLMの準備\n",
    "# llm = OpenAI(temperature=0, verbose=True)\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"../model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The height of Mount Fuji is 3,776 meters (12,421 feet) above sea level.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  8550.53 ms\n",
      "llama_print_timings:      sample time =    24.80 ms /    30 runs   (    0.83 ms per token,  1209.77 tokens per second)\n",
      "llama_print_timings: prompt eval time = 11645.87 ms /    19 tokens (  612.94 ms per token,     1.63 tokens per second)\n",
      "llama_print_timings:        eval time =  9120.24 ms /    29 runs   (  314.49 ms per token,     3.18 tokens per second)\n",
      "llama_print_timings:       total time = 20861.47 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The height of Mount Fuji is 3,776 meters (12,421 feet) above sea level.\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"### Instruction: What is the height of Mount Fuji?\n",
    "### Response:\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:langchain.vectorstores.base:Embeddings is not implemented for FAISS\n"
     ]
    }
   ],
   "source": [
    "# 質問応答チェーンの作成\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=index.as_retriever(search_kwargs={\"k\": 4}),\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Arsenal Football Club is a team in the Premier League."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  8550.53 ms\n",
      "llama_print_timings:      sample time =    11.68 ms /    14 runs   (    0.83 ms per token,  1198.84 tokens per second)\n",
      "llama_print_timings: prompt eval time = 112664.07 ms /   400 tokens (  281.66 ms per token,     3.55 tokens per second)\n",
      "llama_print_timings:        eval time =  4202.81 ms /    14 runs   (  300.20 ms per token,     3.33 tokens per second)\n",
      "llama_print_timings:       total time = 116987.74 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "A1:  Arsenal Football Club is a team in the Premier League.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.09it/s]\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Holloway, North London"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  8550.53 ms\n",
      "llama_print_timings:      sample time =     5.90 ms /     7 runs   (    0.84 ms per token,  1186.64 tokens per second)\n",
      "llama_print_timings: prompt eval time = 78574.01 ms /   278 tokens (  282.64 ms per token,     3.54 tokens per second)\n",
      "llama_print_timings:        eval time =  1765.62 ms /     6 runs   (  294.27 ms per token,     3.40 tokens per second)\n",
      "llama_print_timings:       total time = 80412.88 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "A2:  Holloway, North London\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " According to the context, Arsenal's motto is \"Victoria Concordia Crescit\", which means \"Victory Through Harmony\" in Latin.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "A3:  According to the context, Arsenal's motto is \"Victoria Concordia Crescit\", which means \"Victory Through Harmony\" in Latin.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  8550.53 ms\n",
      "llama_print_timings:      sample time =    31.28 ms /    38 runs   (    0.82 ms per token,  1214.95 tokens per second)\n",
      "llama_print_timings: prompt eval time = 87377.18 ms /   312 tokens (  280.06 ms per token,     3.57 tokens per second)\n",
      "llama_print_timings:        eval time = 10879.43 ms /    37 runs   (  294.04 ms per token,     3.40 tokens per second)\n",
      "llama_print_timings:       total time = 98427.91 ms\n"
     ]
    }
   ],
   "source": [
    "# 質問応答チェーンの実行\n",
    "print(\"A1:\", qa_chain.run(\"What team is Arsenal?\"))\n",
    "print(\"A2:\", qa_chain.run(\"Where is Arsenal based?\"))\n",
    "print(\"A3:\", qa_chain.run(\"What is Arsenal's motto?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
